
algo:

  delayed_training: False

  name: "PPO"

  normalize_advantage: True
  grad_norm: 0.5
  
  gamma: 0.995
  gae_lamda: 0.95

  truncate_grads: True
  normalize_value: True

  value_size: 1 # the amount of values the critic outputs
  weight_decay: 0.0

  learning_rate: 5e-4

  schedule_type: 'standard'
  lr_schedule: adaptive
  kl_threshold: 0.004

  e_clip: 0.2
  clip_value: False

  horizon_length: 32
  minibatch_size: 32768

  mini_epochs: 5

  reward_shaper:
    scale_value: 0.01
    shift_value: 0

  value_bootstrap: True


  critic_coef: 2





