
seed: 42

task:
  name: "02_mk1_walking"
  physics_engine: physx
  env:
    num_envs: 1024
    env_spacing: 10
    max_episode_length: 1000

    motor_strength_factor: 10

    reward_weighting:
      directional_factor: 10
      death_cost: 10
      energy_cost: 0.0005
      alive_reward: 0.4
      torso_torque_cost: 0.005
      upright_punishment_factor: 2 # higher values punish the acotr more for bad posture
    
  sim:
    substeps: 2

  task: 
    randomize: True


policy: 
    name: "SimpleActorCritic"
    
    
    actor_hidden_layers:
      - 128
      - 256
      - 128
      - 64
    critic_hidden_layers:
      - 128
      - 256
      - 128
      - 64
    lr: 3e-4

    


algo:
  name: "PPO" 
  gamma: 0.99
  n_epochs: 8
  eps_clip: 0.2
  n_steps: 32 # the amount of steps taken per env between training and update
  batch_size: 8192
  target_kl: 0.008
  gae_lamda: 0.95
  entropy_coef: 0
  value_f_coef: 4
