
seed: 42

task:
  name: "02_mk1_walking"

  vec_task: {}

  mk1: {}

  mk1_walking: {}


policy: 
    name: "SimpleActorCritic"
    is_std_param: False
    # init_log_std: 0.0  # Only makes sence, when the is_std_param is true
    log_std:  # only makes sence, if the is_std_param is false
      interpolation: 'linear'
      schedule:
        - [0, 0.0]
        - [1e8, -0.1]
        - [2e8, -0.2]
        - [4e8, -0.3]
        - [6e8, -0.4]    
        - [8e8, -0.5]    
        - [10e8, -0.6]    
        - [12e8, -0.7]    
        - [14e8, -0.8]    
        - [15e8, -1.0]    
        - [17e8, -1.3]    

    
    actor_hidden_layers:
      - 128
      - 256
      - 128
      - 64
    critic_hidden_layers:
      - 128
      - 256
      - 128
      - 64
    lr: 3e-4


algo:
  name: "PPO" 
  gamma: 0.99
  n_epochs: 8
  eps_clip: 0.2
  n_steps: 128 # the amount of steps taken per env between training and update
  batch_size: 8192
  target_kl: 0.008
  gae_lamda: 0.95
  entropy_coef: 0
  value_f_coef: 4
