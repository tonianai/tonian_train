
seed: 42

task:
  name: "02_mk1_walking"
  physics_engine: physx
  env:
    num_envs: 1024
    env_spacing: 10
    max_episode_length: 1000
    motor_strength_factor: 10

    initial_velocities: 
      - 0
      - -1
      - 0


    reward_weighting:
      death_height: 1.25 # the height of the 
      directional_factor: 0.8
      death_cost: 10
      energy_cost: 0.005
      alive_reward: 0.7
      upright_punishment_factor: 16 # higher values punish the actor more for bad posture
      jitter_cost: 0.25


    agent:

      default_motor_power: 1000 

      motor_powers: # describe by using joint name and than the max efort in NM
        left_foot: 300
        left_knee: 1500
        left_hip_b: 1000
        left_hip_a: 1000
        
        right_foot: 300
        right_knee: 1500
        right_hip_b: 500
        right_hip_a: 500

        torso: 300

        left_elbow: 400
        left_arm_rotate: 200
        left_shoulder_b: 400
        left_shoulder_a: 400


        right_elbow: 400
        right_arm_rotate: 200
        right_shoulder_b: 400
        right_shoulder_a: 400




  sim:
    substeps: 2


  task: 
    randomize: True
    randomization_params: 
      sim_params: 
        gravity:
          range: [0, 0.4]
          operation: "additive"
          distribution: "gaussian"
          schedule: "linear"  # "linear" will linearly interpolate between no rand and max rand
          schedule_steps: 3000
      actor_params:
        anymal:
          color: True
          rigid_body_properties:
            mass: 
              range: [0.5, 1.5]
              operation: "scaling"
              distribution: "uniform"
              setup_only: True # Property will only be randomized once before simulation is started. See Domain Randomization Documentation for more info.
              schedule: "linear"  # "linear" will linearly interpolate between no rand and max rand
              schedule_steps: 3000
          rigid_shape_properties:
            friction:
              num_buckets: 500
              range: [0.7, 1.3]
              operation: "scaling"
              distribution: "uniform"
              schedule: "linear"  # "linear" will scale the current random sample by `min(current num steps, schedule_steps) / schedule_steps`
              schedule_steps: 3000
            restitution:
              range: [0., 0.7]
              operation: "scaling"
              distribution: "uniform"
              schedule: "linear"  # "linear" will scale the current random sample by `min(current num steps, schedule_steps) / schedule_steps`
              schedule_steps: 3000
          dof_properties:
            damping: 
              range: [0.5, 1.5]
              operation: "scaling"
              distribution: "uniform"
              schedule: "linear"  # "linear" will scale the current random sample by `min(current num steps, schedule_steps) / schedule_steps`
              schedule_steps: 3000
            stiffness: 
              range: [0.5, 1.5]
              operation: "scaling"
              distribution: "uniform"
              schedule: "linear"  # "linear" will scale the current random sample by `min(current num steps, schedule_steps) / schedule_steps`
              schedule_steps: 3000
            lower:
              range: [0, 0.01]
              operation: "additive"
              distribution: "gaussian"
              schedule: "linear"  # "linear" will scale the current random sample by `min(current num steps, schedule_steps) / schedule_steps`
              schedule_steps: 3000
            upper:
              range: [0, 0.01]
              operation: "additive"
              distribution: "gaussian"
              schedule: "linear"  # "linear" will scale the current random sample by `min(current num steps, schedule_steps) / schedule_steps`
              schedule_steps: 3000


policy: 
    name: "SimpleActorCritic"
    
    
    actor_hidden_layers:
      - 128
      - 256
      - 128
      - 64
    critic_hidden_layers:
      - 128
      - 256
      - 128
      - 64
    lr: 3e-4

    


algo:
  name: "PPO" 
  gamma: 0.99
  n_epochs: 8
  eps_clip: 0.2
  n_steps: 128 # the amount of steps taken per env between training and update
  batch_size: 8192
  target_kl: 0.008
  gae_lamda: 0.95
  entropy_coef: 0
  value_f_coef: 4
