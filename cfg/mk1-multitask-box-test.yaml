
seed: 42

#start_model: 'start_models/02_mk1_walking.pth'

task:
  name: "05_mk1_multitask_box"

  vec_task: 
    num_envs: 1024
    

  mk1: {}

  mk1_multitask:

    reward_weighting:

        to_target_prob:  1.00
        to_target:
          death_height: 1.25 # the height of the base at which the episode is terminated and the actor is considered dead
          death_cost: 10      
          forward_directional_factor: 0.4
          zero_clip_direction_reward: True
          energy_cost: 0.005
          alive_reward: 2
          upright_punishment_factor: 20 # higher values punish the actor more for bad posture
          jitter_cost: 0.25
          overextend_cost: 0.4

          to_target_directional_factor: 0.8 # reward for the forward velocity facing the target position
          target_reached_reward_factor: 100 


          velocity_reward_factor: 0.0

          die_on_contact: False # die when anything other than the foot touches the ground
          contact_punishment: 0 # punsihment if anyhing other than feet touvh the ground only usefull if die_on_contact is of

        idle_prob: 0.00
        idle:
          death_height: 1.25 # the height of the base at which the episode is terminated and the actor is considered dead
          death_cost: 160      
          forward_directional_factor: 0.0
          zero_clip_direction_reward: True
          energy_cost: 0.01
          alive_reward: 5
          upright_punishment_factor: 20 # higher values punish the actor more for bad posture
          jitter_cost: 0.50
          overextend_cost: 0.1
          velocity_reward_factor: -3.0


          die_on_contact: True # die when anything other than the foot touches the ground
          contact_punishment: 3 # punsihment if anyhing other than feet touvh the ground only usefull if die_on_contact is of



policy: 

  normalize_input: True

  actor_net:
    - name: actor_linear_obs_net
      input: 
          - obs: linear
          - obs: state
          - obs: target_pos
      mlp:
          units: [512 ,256, 128]
          activation: elu
          initializer: default



algo:
  name: "PPO"
  grad_norm: 1.0
  gamma: 0.99
  mixed_precision: True
  gae_lambda: 0.95
  truncate_grads: True
  normalize_value: True 
  normalize_advantage: True
  value_size: 1 # the amount of values the critic outputs
  weight_decay: 0.0
  learning_rate: 5e-4
  lr_schedule: adaptive
  kl_threshold: 0.008
  e_clip: 0.2
  clip_value: True
  horizon_length: 32
  minibatch_size: 32768
  mini_epochs: 5
  reward_shaper:
    scale_value: 0.01
    shift_value: 0

  value_bootstrap: True
  critic_coef: 4
  entropy_coef: 0

  bounds_loss_coef: 0.0001



