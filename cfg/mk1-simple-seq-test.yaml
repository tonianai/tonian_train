
seed: 42

name: "sequential-mk1"
#start_model: 'start_models/02_mk1_walking.pth'

task:
  name: "02_mk1_running"

  vec_task: 
    num_envs: 1024
    



policy:
    sequence_length: 1 # the amount of steps the agent can look back 
    normalize_input: True
 
    network:
        network_type: "simple_sequential"   
        is_std_fixed: false
        policy_type: a2c_transformer
        normalize_input: true
        main_body:  
         - name: 'main_body'
           input: 
              - obs: linear 
           mlp:
              units: [512, 512,  256, 128]
              activation: elu
              initializer: default
        
         
        action_head:
            units:
                - 128
                - 64
            activation: relu
            initializer: default
        critic_head:
            units:
                - 128
                - 64
            activation: relu
            initializer: default


algo:
  name: "PPO"
  grad_norm: 1.0
  gamma: 0.99
  mixed_precision: True
  gae_lambda: 0.95
  truncate_grads: True
  normalize_value: True 
  normalize_advantage: True
  value_size: 1 # the amount of values the critic outputs
  weight_decay: 0.0
  learning_rate: 5e-4
  lr_schedule: adaptive
  kl_threshold: 0.008
  e_clip: 0.2
  clip_value: True
  horizon_length: 32 # the amount of steps per epoch
  minibatch_size: max
  mini_epochs: 2
  reward_shaper:
    scale_value: 0.01
    shift_value: 0

  value_bootstrap: True
  critic_coef: 4
  entropy_coef: 0

  bounds_loss_coef: 0.0001





